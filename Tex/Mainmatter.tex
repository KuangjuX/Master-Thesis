%---------------------------------------------------------------------------%
%->> Main content
%---------------------------------------------------------------------------%
\section{选题的背景及意义}


机器学习、图形处理和图像处理等数据密集型应用在人工智能、大数据和计算机视觉等各个领域中变得越来越重要。这些应用通常涉及复杂的数据结构，例如张量、矩阵、图形和图像，并且需要高性能和可扩展的计算和数据移动。然而，由于数据结构、算法以及目标架构和平台的多样性和复杂性，开发和优化此类应用程序具有挑战性。编译器、库和框架等传统方法通常无法捕获数据密集型应用程序的高级语义和低级细节，因此无法实现所需的性能和可扩展性。尤其是在大语言模型的流行以及 DNN 算法的发展，各种创新的 DNN 算法难以被描述为大型的、规则的线性代数运算。

优化数据密集型应用程序的关键挑战之一是执行细粒度融合和循环优化。细粒度融合是指将多个操作组合成单个操作的过程，这可以减少数据移动和应用程序的内存占用。循环优化是指转换应用程序循环的过程，例如循环融合、平铺、倾斜和并行化，这可以改善数据局部性、并行性和应用程序的性能。但是，执行细粒度融合和循环优化很困难。

首先，数据密集型应用中的操作粒度往往过大，限制了融合和循环变换的机会。例如神经网络中的卷积运算可以看作是单个运算，也可以看作是多个运算的组合，如矩阵乘法、元素加法、激活函数等。粒度的选择会影响应用程序的性能和可扩展性。

其次，数据密集型应用程序的数据依赖关系和内存层次结构通常非常复杂且动态，这使得分析和优化应用程序的数据移动和数据局部性变得非常困难。例如，图处理应用程序可以根据输入图、算法和平台具有不同的数据依赖关系和内存层次结构。由于图的动态特性，数据依赖关系和内存层次结构也会在应用程序执行过程中发生变化。

最后，数据密集型应用的循环访问模式往往是不规则和非仿射的，这使得现有的循环分析和变换技术（如多面体模型）难以应用。例如，稀疏矩阵乘法可以具有不规则和非仿射的循环访问模式，这取决于矩阵的稀疏性和结构。现有的循环分析和变换技术通常假设循环访问模式是规则和仿射的，因此无法处理不规则和非仿射的情况。

因此，设计一种方法能够表示当前各种创新的 DNN 算法，同时能够充分利用硬件特性并生成有效率的可在硬件执行的代码具有十分重要的意义，也是本文研究的课题。


\section{国内外本学科领域的发展现状与趋势}


% \subsection{Pytorch}

% \subsection{Triton}

\subsection{深度学习编译器的发展现状}

当前的深度学习软件栈往往将 DNN 算法抽象成 DAG（有向无环图） 来描述，在早期这种方法表示方法可以用于扩展算法的并行度，然而随着 DNN 算法的发展，使用 DAG 的表示方法对于当前的 DNN 算法带来了限制。各种创新的 DNN 算法不能被描述为大型的、规则的线性代数运算。例如 FlashAttention~\citepns{dao2022flashattention, dao2023flashattention, shah2024flashattention} 是一种硬件感知算法，在共享内存以及寄存器内存引入了 online softmax~\citepns{milakov2018online} 代替 softmax，从而可以在共享内存以及寄存器进行数据复用。然而基于 DAG 的表示方法难以表达复杂的硬件感知算法。

\subsubsection{端到端的深度学习编译器}

TVM~\citepns{tvm/chen2018tvm} 是一个端到端的深度学习编译器。在 TVM 中采用低级循环程序合成技术对性能进行微调~\citepns{tvm/zheng2020ansor}，采用类似爱因斯坦求和的张量表达语言来详细说明张量运算符如何执行数值运算~\citepns{tvm/lai2023relax, tvm/roesch2018relay}。

在 TVM 中引入了基于搜索的自动调优方法，AutoTVM 可以通过事先编写模版来组成调度的搜索空间，然而这对模版的编写带来了很高的要求，Ansor 更进一步可以自动生成一个覆盖全面的优化搜索空间并为每个张量程序提供被选择的机会，同时通过 Auto-tunning 得到性能最好的模版。尽管这种方法对于用户来说很容易使用，但编译器的自动调度可能会导致高复杂性，张量表达式在内部很快被转换为循环表达，导致抽象很快丢失。


然而自动调优技术无法对于硬件进行感知，只能优化代码在通用计算核心运行，通常无法发挥出硬件的最大潜力，从而导致性能不佳。为了解决自动调优对于硬件感知不够的问题，TVM 尝试引入具有临时语义的算子与硬件模版库，例如 BOLT 通过使用 CUTLASS~\citepns{CUTLASS} 手写融合矩阵乘及卷积算法，在共享内存以及寄存器内存中进行数据复用，从而减少算子间加载和存储内存的次数，减少内核启动的消耗以及扩大优化范围以探索更多指令调度。

然而，基于临时语义的解决方案很难应对不断发展的 DNN 算法，并且基于临时语义的解决方案也可能破环整体软件栈的概念性，使得其变得极为臃肿与复杂。

\begin{figure}%[h]
    \centering
    \includegraphics[width=\textwidth]{figures/AMOS.png}
    \caption{System overview of AMOS}
    \label{figure:AMOS}
\end{figure}

AMOS~\citepns{isca/zheng2022amos} 中提出了一种更加通用的 IR 来充分利用硬件特性，如图\ref{figure:AMOS}所示。AMOS 将计算加速器从软件到硬件的映射方式分为硬件感知和指令集感知。硬件感知知晓硬件架构的细节信息，比如 PE 数量，PE 间互联方式，这样软硬件映射过程可以转化为一个在给定硬件约束下的优化问题；指令集感知是映射过程对于硬件架构无感知，只根据对外暴露的指令集进行软硬件映射。

AMOS 通过引入的新的 IR，进一步提出了计算、内存抽象，输入是由 Python 编写的高层次代码和基于计算和方寸抽象出的硬件指令，然后生成不同的映射组合，一个映射组合由计算映射和访问映射组成。计算映射是将算子转换为计算指令映射到加速器上；访存映射通过访存指令描述数据的访问和存储行为。AMOS 的映射过程分为两个阶段，第一阶段将循环迭代映射到虚拟加速器上，第二阶段加入硬件资源和指令集限制，调整映射过程并将其映射到真实的物理加速器上。

尽管 AMOS 在很大程度上引入了编译器对于硬件的感知，但是在 DSL 层面没有引入硬件特性，基于分析的优化方法依然无法完全发挥出硬件的性能。同时将 AMOS 引入 TVM 等端到端的深度学习编译器依然是一个 ad-hoc 的方法，尽管这可能在某些领域有效但在某些方面可能会失去效果甚至变得更差，同时也会破坏开源软件栈的一致性。

\subsubsection{基于多面体模型的编译器}

\subsubsection{硬件感知的深度学习编译器}

\subsection{未来发展趋势}





\section{课题主要研究内容、预期目标}



\section{拟采用的研究方法、技术路线、实验方案及其可行性分析}



\section{已有科研基础与所需的科研条件}


\section{研究工作计划与进度安排}



\section*{填表说明}

本表内容须真实、完整、准确。

\section*{常见使用问题}

\begin{enumerate}
    \item 模板使用说明请见 \href{https://github.com/mohuangrui/ucasthesis}{ucasthesis：中国科学院大学学位论文 LaTeX 模板}.
    \item 填表说明和模板说明不是开题报告的一部分，请删除。
    \item 开题报告样式设计导致对题目换行与不换行难以兼容，排版十分困难。推荐采用当前设置，尽量避免将精力花在这些无关紧要的细节上。
\end{enumerate}

\nocite{*}% 使文献列表显示所有参考文献（包括未引用文献）
%---------------------------------------------------------------------------%
