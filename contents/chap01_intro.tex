\section{绪论}

\subsection{研究背景及意义}

深度学习 (Deep Learning) 的发展对各个科学领域产生了深远的影响。
它不仅在自然语言处理 (NLP) 和计算机视觉 (CV) 等人工智能领域展现出非凡的价值，而且在电子商务、智慧城市和药物研发等更广泛的应用中也取得了巨大成功。
随着卷积神经网络 (CNN)~\cite{lecun1998cnn}、循环神经网络 (RNN)~\cite{rumelhart1986rnn}、长短期记忆 (LSTM)~\cite{hochreiter1997lstm} 和生成对抗网络 (GAN)~\cite{goodfellow2020gan} 等多功能深度学习模型的出现，简化各种 DL 模型的编程对于实现其广泛应用至关重要。
随着工业界和学术界的不断努力，为了简化各种深度学习模型的实现，已经提出了几种流行的深度学习框架，例如 TensorFlow~\cite{osdi/abadi2016tensorflow}、PyTorch~\cite{nips/paszke2019pytorch}、MXNet~\cite{arxiv/chen2015mxnet} 等。
同时，矩阵乘法等独特的计算特性激发了芯片架构师设计定制深度学习加速器的热情，以提高效率。例如 Google TPU、NVIDIA GPU、海思 NPU 等。
除此之外，为了适应硬件的多样性，将计算有效地映射到深度学习硬件非常重要。
在通用硬件上，高度优化的线性代数库（例如基本线性代数子程序 (BLAS) 库）（例如 MKL 和 cuBLAS）是深度学习模型高效计算的基础。
以卷积操作为例，深度学习框架将卷积转换为矩阵乘法，然后调用 BLAS 库中的 GEMM 函数。

为了解决以上问题，深度学习社区提出了使用特定领域的编译器来解决问题。
很快，工业界和学术界都提出了几种流行的深度学习编译器，例如 TVM~\cite{tvm/chen2018tvm}、XLA~\cite{xla} 等。
深度学习编译器将深度学习框架中的模型定义作为输入，针对模型规范和硬件架构，模型定义和特定代码实现之间的转换得到了高度优化。

然而，随着大语言模型（LLM）的出现与流行，当前的深度学习算法设计逐渐更加注重于模型与硬件的协同设计。
例如 Flash Attention~\cite{dao2022flashattention, dao2023flashattention, arxiv/graef2024flash}，Linear Attention~\cite{arxiv/yang2023gated} 等，难以被描述为大型的、规则的线性代数运算。
当前的深度学习编译器通常使用有向无环图来描述算法，在早期这种表示方法可以用于扩展算法的并行度。
然而基于有向无环图的表示方法无法对嵌套循环以及硬件信息进行描述，从而对非规则的硬件感知算法的表达与优化带来了限制。
因此 Triton~\cite{pldi/tillet2019triton} 的出现能够给予 GPU 编程模型允许用户在多个内存层级上进行编程并做自动优化。
然而 Triton 的编程模型过于耦合 NVIDIA GPU 同时无法基于寄存器进行编程，这也带来了一定的限制。

因此，本研究尝试提出一种能够表达当前硬件感知算法的表示方法并能够针对特定的硬件进行优化，最终生成有效率的硬件代码，这在当今的时代有着重要意义。

\subsection{研究内容及主要工作}

\begin{figure}%[h]
    \centering
    \includegraphics[width=\textwidth]{figures/intro/abstract.pdf}
    \caption{本研究系统总体路线}
    \label{figure:abstract}
\end{figure}

为了克服当前深度学习编译器对于硬件感知算法表达困难以及难以进行硬件相关优化的挑战，本研究希望能够提出一种硬件可感知的抽象。
在这层抽象上会尽可能暴露出性能有关的信息，这要起用户去基于特定硬件进行编程。
随后这层抽象将被被分析最后生成硬件上有效率的代码。
本研究的目标希望是通过以上抽象所映射出的硬件代码能够比当前深度学习编译器所生成出的代码性能更好。

本研究的总体路线如图 ~\ref{figure:abstract} 所示。
在最上层基于本研究提供的数据流图 API 使用调度中间表示进行描述，随后进行数据流图构建。
本研究提出了 AffineGraph 用于表示携带嵌套循环信息的数据流图，随后数据流图进行图降低（Graph Lower）将其转换为携带硬件内存信息的数据流图。
随后对图中的不同层级内存进行切块以增大数据重用和减少内存拷贝操作，最终针对具体硬件进行硬件映射与代码生成。
因此，本文的研究内容主要涉及三个方面，即：

\begin{itemize}
    \item 基于多内存层级数据流图的硬件感知抽象
    \item 基于分块策略的内核融合
    \item 高效的内核映射策略
\end{itemize}

\subsubsection{基于多内存层级数据流图的抽象}

当前，以 DAG 为基础的深度学习编译器出现的契机在于不断迭代的新模型架构的出现，例如 CNN~\cite{lecun1998cnn}，RNN~\cite{rumelhart1986rnn}，LSTM~\cite{hochreiter1997lstm} 以及 GAN~\cite{goodfellow2020gan} 等。
然而随着大语言模型的流行，当前的模型逐渐收敛到以 Transformer~\cite{nips/vaswani2017attention} 为核心的算法设计。
当前的算法设计更加注重于硬件感知度以及算法-硬件协同设计。
然而，基于 DAG 粗粒度的表示方法难以对这类算法进行描述。
因此需要一种合适粒度的抽象来对硬件感知算法进行描述，同时，基于 DAG 的描述方法也无法对硬件性能通过软件的分析方法挖掘到极致。
本课题的一个研究目标为设计一套抽象来对这种算法进行描述。

\textbf{预期目标：} 设计一个基于多内存层级数据流图的抽象，能够准确表达出硬件感知性算法并便于进行进一步的优化。

\subsubsection{基于分块策略的内核融合策略}

当前内核融合技术逐渐从基于算子的融合技术走向了基于 Tile 的融合技术。
其中 FlashAttention-v2~\cite{dao2023flashattention} 是一个在学术界和工业界都十分成功的例子。
FlashAttention-v2 通过 Tile 的融合技术将矩阵 Q, K, V 进行切分并不断加载到共享内存中并再次切块加载入寄存器内存中，随后执行在 Tensor Core 上的矩阵乘计算计算并进行归一化。
FlashAttention-v2 相比于传统的多头 Attention 具有极大的性能优势，增加了内存复用与并行度，减少了内存压力。
更进一步，FlashDecoding 通过将 K，V 进行进一步的切分并将其派发到不同的 block 中进行运行更进一步提高了并行度并可以将其运行在推理中。
除此之外，基于分块策略的内核融合技术也在其他算法中被应用，例如，FlashFFTConv~\cite{arxiv/fu2023flashfftconv} 探讨了如何基于分块融合对长卷积进行优化。
FlashNorm~\cite{arxiv/graef2024flash} 则探讨了如何对 RMSNorm 基于分块融合进行优化。

然而，基于 Tile 的内核融合技术在深度学习编译器中的应用仍然不是很多。
尽管 Roller~\cite{osdi/zhu2022roller}，Welder~\cite{osdi/shi2023welder}，PIT~\cite{sosp/zheng2023pit} 针对基于 Tile 的内核融合技术给出了一些尝试，然而它们依然是在基于 DAG 的编译器例如 TVM~\cite{tvm/chen2018tvm} 中进行讨论与实现。
然而由于 DAG 自身的局限性，基于 DAG 的 tilling 并没有办法去发挥所有硬件潜能。

因此，本课题的一个研究目标在于如何基于硬件感知的情况下进一步通过分块策略进行内核融合并进一步挖掘硬件性能。

\textbf{预期目标：} 将基于 Tile 的内核融合技术应用在多存层级的数据流图上进行优化，使得能够具有更多的内存复用以及挖掘出最大并行度。

\subsubsection{高效的内核映射策略}

当前的深度学习编译器通常将后端代码生成映射到硬件库（例如 CuBLAS, CuDNN~\cite{chetlur2014cudnn} 等）或者是基于手写的 CUDA Kernel 并在自动调优的协同下去自动生成。
然而硬件厂商库尽管在单算子上具有较好的性能，但是由于是黑盒，因此无法处理内核融合的问题；而基于手写与自动调优的 CUDA Kernel 则在一些情况下很难与优化的非常好的硬件厂商库的性能进行抗衡。

尽管 CUTLASS~\cite{CUTLASS} 提供了一个基于模版的编程框架，将一系列内核组件进行拆分，提供了 TiledMMA，TiledCopy，MMAAtom 等抽象。
然而，CUTLASS 过于极致的模版技术的运用以及非常灵活的抽象设计导致了编译器难以基于 CUTLASS 做分析及做代码生成。

ThunderKittens~\cite{arxiv/spector2024thunderkittens} 提供了一个合适的抽象粒度，它以线程束作为一个基准线，在线程束下的实现部分作为黑盒实现，在线程束之上作为白盒可以由用户进行设计。
然而，ThunderKittens 的限制在于它仅仅针对 H100 做了优化并且无法给予不同切块策略进行自动调优。
同时，由于 NVIDIA 硬件的高度复杂性，用户也很难针对自己的需求对 ThunderKittens 针对特定硬件进行优化与修改。

因此，本课题希望设计一套具有合适粒度抽象以及能够支持性能调优的代码生成方案。

\textbf{预期目标：} 基于 Tile 内核融合的中间表示设计一套具有合适粒度抽象以及能够支持性能调优的代码生成方案并进行有效率的代码生成。
生成的代码性能能够比当前主流的深度学习编译器要更好。

\subsection{本文组织结构}

本文共分为六章，各章节内容安排如下：

第一章：绪论。介绍研究背景、意义、主要研究内容及本文的组织结构。

第二章：相关技术概述。综述当前深度学习编译器的发展现状，分析现有技术（如基于DAG、多面体模型、硬件感知编译器等）的优缺点，并指出存在的问题。

第三章：基于多内存层级数据流图的硬件感知抽象。详细介绍 AffineGraph IR 的设计，包括缓冲区节点、运算符节点、子图节点、仿射边、仿射映射及谓词执行等核心组件，并以 FlashAttention-2 为例展示 IR 的表达能力。

第四章：基于分块策略的内核融合。阐述硬件感知优化策略，包括分块大小与并行度确定、解析式代价模型构建、Warp Layout 影响分析、层级执行模型以及高效内存访问优化。同时论述面向 NVIDIA Hopper 架构的可扩展性设计。

第五章：高效的内核映射策略。介绍图降低与代码生成算法，以 GEMM 为案例展示完整的编译流程，包括层级化 IR 构建、布局推断与变换、CUDA 代码生成。同时展示系统实现细节，并给出在 NVIDIA A100 上涵盖内存操作、GEMM、融合 GEMM、FlashAttention 和 Block Sparse Attention 的全面实验评估，以及在 NVIDIA H800（Hopper 架构）上的可移植性验证实验。

第六章：总结和展望。总结本文的主要工作与贡献，并对未来的研究方向进行展望。
