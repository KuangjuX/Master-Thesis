\section{绪论}

\subsection{研究背景及意义}

\subsubsection{深度学习的发展与计算需求}

深度学习 (Deep Learning) 的发展对各个科学领域产生了深远的影响。
它不仅在自然语言处理 (NLP) 和计算机视觉 (CV) 等人工智能领域展现出非凡的价值，而且在电子商务、智慧城市和药物研发等更广泛的应用中也取得了巨大成功。
随着卷积神经网络 (CNN)~\cite{lecun1998cnn}、循环神经网络 (RNN)~\cite{rumelhart1986rnn}、长短期记忆 (LSTM)~\cite{hochreiter1997lstm} 和生成对抗网络 (GAN)~\cite{goodfellow2020gan} 等多功能深度学习模型的出现，简化各种 DL 模型的编程对于实现其广泛应用至关重要。
随着工业界和学术界的不断努力，为了简化各种深度学习模型的实现，已经提出了几种流行的深度学习框架，例如 TensorFlow~\cite{osdi/abadi2016tensorflow}、PyTorch~\cite{nips/paszke2019pytorch}、MXNet~\cite{arxiv/chen2015mxnet} 等。
同时，矩阵乘法等独特的计算特性激发了芯片架构师设计定制深度学习加速器的热情，以提高效率。例如 Google TPU、NVIDIA GPU、海思 NPU 等。
除此之外，为了适应硬件的多样性，将计算有效地映射到深度学习硬件非常重要。
在通用硬件上，高度优化的线性代数库（例如基本线性代数子程序 (BLAS) 库）（例如 MKL 和 cuBLAS）是深度学习模型高效计算的基础。
以卷积操作为例，深度学习框架将卷积转换为矩阵乘法，然后调用 BLAS 库中的 GEMM 函数。

\subsubsection{深度学习编译器的兴起与演进}

为了解决深度学习模型在多样化硬件上高效部署的问题，深度学习社区提出了使用特定领域的编译器来解决问题。
深度学习编译器的发展经历了几个重要阶段。

\paragraph{基于图的编译优化}
早期的深度学习编译器以有向无环图（DAG）作为核心中间表示。
工业界和学术界提出了几种流行的深度学习编译器，例如 TVM~\cite{tvm/chen2018tvm}、XLA~\cite{xla} 等。
这些编译器将深度学习框架中的模型定义作为输入，将计算图中的算子视为不透明的节点，通过图级优化（如算子融合、常量折叠、布局变换等）和算子级优化（如循环分块、向量化等）来提升执行效率。
TVM~\cite{tvm/chen2018tvm} 是这一阶段的代表性工作，它提出了基于张量表达式的调度语言，并通过 AutoTVM~\cite{nips/chen2018learning} 和 Ansor~\cite{tvm/zheng2020ansor} 等自动调优框架来搜索最优的算子实现。
XLA~\cite{xla} 则作为 TensorFlow 的编译后端，通过 HLO（High-Level Optimizer）中间表示进行图级和算子级的联合优化。
这一阶段的编译器有效地解决了算子调度和基本融合问题，但其 DAG 表示的粗粒度特性限制了对底层硬件特性的精细利用。

\paragraph{多面体模型与循环优化}
为了克服 DAG 表示在循环变换方面的局限性，以 Tiramisu~\cite{cgo/baghdadi2019tiramisu}、AKG~\cite{pldi/zhao2021akg} 和 FreeTensor~\cite{pldi/tang2022freetensor} 为代表的编译器引入了多面体模型（Polyhedral Model）。
多面体模型使用仿射不等式描述循环嵌套的迭代空间，通过线性规划和仿射变换来自动发现最优的循环变换策略~\cite{TPDS/li2020deep}。
这一方法在理论上具有强大的表达力和优化能力，能够自动进行循环交换、循环分块、循环融合等复杂变换。
然而，多面体模型的灵活性也带来了挑战：其调度空间过于庞大，难以与自动调优机制有效集成；同时，多面体编译器缺乏对 GPU 等特定硬件（如 Tensor Core、共享内存 Bank Conflict）的原生抽象，导致生成代码的质量难以与手写优化代码匹敌。

\paragraph{硬件感知编译与 Tile 抽象}
随着 GPU 架构的不断演进，以 Triton~\cite{pldi/tillet2019triton} 为代表的硬件感知编译器应运而生。
Triton 在 DSL 中引入了 \texttt{load}、\texttt{store}、\texttt{dot} 等以 GPU 为核心设计的运算符，允许用户在共享内存层级上进行编程，并通过编译器自动完成底层优化。
近年来，以 Tile（分块）为核心抽象的编程框架进一步涌现，包括 CuTe~\cite{cute}、ThunderKittens~\cite{arxiv/spector2024thunderkittens}、TileLang~\cite{arxiv/wang2025tilelang} 等。
这些框架将 Tile 作为一等公民，在不同内存层级之间显式管理数据分块与移动，在保持较高抽象层次的同时充分发挥硬件性能。
然而，这些框架在设计上各有侧重：Triton 隐式管理共享内存，无法在寄存器级别进行精细控制；ThunderKittens 仅针对特定架构优化；TileLang 构建于 TVM 之上，受限于其底层基础设施。
目前尚缺乏一个统一的抽象框架能够同时涵盖层次化的内存管理、精确的数据访问模式分析、自动化的调度优化和跨架构的可移植性。

\subsubsection{大语言模型对编译器的新挑战}

近年来，大语言模型（Large Language Model, LLM）的出现与流行为深度学习编译器带来了全新的挑战。
以 GPT~\cite{nips/vaswani2017attention}、Llama、Qwen 等为代表的大语言模型，其参数规模从数十亿到数千亿不等，对计算和内存资源提出了前所未有的需求。
与早期多样化的模型架构（CNN、RNN、GAN 等）不同，当前的模型逐渐收敛到以 Transformer~\cite{nips/vaswani2017attention} 为核心的架构设计，但在算子层面呈现出更高的复杂性。

\paragraph{硬件感知算法的兴起}
当前的深度学习算法设计逐渐更加注重于模型与硬件的协同设计。
FlashAttention~\cite{dao2022flashattention, dao2023flashattention, arxiv/graef2024flash} 是这一趋势的典型代表：它通过精细控制数据在全局内存、共享内存和寄存器之间的流动，将标准注意力机制的内存复杂度从 $O(N^2)$ 降低到 $O(N)$，同时实现了显著的计算加速。
类似地，Linear Attention~\cite{arxiv/yang2023gated}、Mamba~\cite{arxiv/dao2024transformers} 等新兴架构也采用了硬件感知的设计理念，通过算法层面的创新来适应 GPU 的内存层级结构。
这些硬件感知算法难以被描述为大型的、规则的线性代数运算，它们涉及复杂的嵌套循环结构、多层级的数据流动模式和条件控制流，对编译器的表达力和优化能力提出了更高的要求。

\paragraph{计算特征的变化}
大语言模型的计算特征也发生了显著变化。
在训练阶段，矩阵乘法（GEMM）仍然是计算密集型的核心操作，但其形状更加多样化（如长序列带来的非方阵 GEMM）。
在推理阶段，由于自回归生成的特性，计算模式从计算密集型（Compute-bound）转变为内存密集型（Memory-bound），KV Cache 的管理、小批量 GEMM 和逐元素操作（如 RMSNorm、Softmax）的优化变得至关重要。
此外，混合专家模型（Mixture of Experts, MoE）引入了稀疏计算模式，Block Sparse Attention 等技术通过选择性计算来降低复杂度。
这些多样化的计算特征要求编译器能够同时高效处理计算密集型和内存密集型操作，并支持不规则的访问模式。

\subsubsection{GPU 架构演进与编译器的协同}

GPU 架构的快速演进为深度学习编译器带来了新的机遇和挑战。
NVIDIA GPU 从 Volta 架构开始引入 Tensor Core，此后每一代架构都引入了新的硬件特性，对编译器的适配能力提出了更高的要求。

\paragraph{Volta/Turing 架构（2017--2019）}
Volta 架构首次引入了 Tensor Core，支持 FP16 的矩阵乘累加（MMA）操作，为深度学习推理和训练提供了专用的计算单元。
Turing 架构进一步增加了 INT8 和 INT4 的 Tensor Core 支持，为量化推理提供了硬件基础。
这一时期的编译器主要通过调用 cuBLAS 和 cuDNN 等厂商库来利用 Tensor Core，编译器本身对 Tensor Core 的直接支持有限。

\paragraph{Ampere 架构（2020）}
Ampere 架构（以 A100 GPU 为代表）引入了多项关键特性：（1）第三代 Tensor Core，支持 TF32、BF16 等新数据类型；（2）异步数据拷贝指令 \texttt{cp.async}，允许数据从全局内存直接拷贝到共享内存而不经过寄存器；（3）增大的共享内存容量（每 SM 最多 164KB）。
这些特性使得软件流水线和多级缓冲成为可能，但也增加了编译器在指令选择和同步管理方面的复杂性。

\paragraph{Hopper 架构（2022）}
Hopper 架构（以 H100/H800 GPU 为代表）引入了更为激进的硬件创新：（1）张量内存加速器（TMA），提供硬件加速的多维数据传输；（2）线程块集群（Thread Block Clusters），允许集群内的线程块直接访问彼此的共享内存；（3）Warpgroup 级操作（\texttt{wgmma}），支持 128 线程的集合矩阵运算。
这些特性进一步拉大了硬件能力与编译器利用能力之间的差距——手写优化代码可以充分利用这些新特性，但现有编译器往往难以自动生成同等质量的代码。

GPU 架构的快速演进意味着编译器不仅需要生成高效的代码，还需要具备良好的\textbf{架构可扩展性}——核心抽象和优化策略应能够自然地适配新的硬件特性，而无需对编译器进行根本性的重新设计。
这一需求是本课题设计 AffineGraph IR 时的重要考量之一。

\subsubsection{研究意义}

综合以上分析，当前深度学习编译器领域面临三个核心挑战：
（1）\textbf{表达力不足}——现有 IR 难以精确描述硬件感知算法中复杂的多层级数据流动模式；
（2）\textbf{硬件利用不充分}——编译器生成的代码难以充分利用 Tensor Core、异步数据传输等硬件特性；
（3）\textbf{架构可扩展性差}——针对特定架构的优化难以迁移到新的 GPU 代际。

因此，本研究尝试提出一种能够表达当前硬件感知算法的表示方法并能够针对特定的硬件进行优化，最终生成有效率的硬件代码。
具体而言，本研究提出了 AffineGraph——一种基于多内存层级数据流图的编译器框架，通过仿射变换精确建模内存访问模式和计算依赖关系，为硬件感知算法的表达和优化提供统一的抽象基础。
该研究对于推动深度学习编译器技术的发展、缩小编译器生成代码与手写优化代码之间的性能差距、以及降低高性能 GPU 编程的门槛，具有重要的理论意义和实用价值。

\subsection{研究内容及主要工作}

\begin{figure}%[h]
    \centering
    \includegraphics[width=\textwidth]{figures/intro/abstract.pdf}
    \caption{本研究系统总体路线}
    \label{figure:abstract}
\end{figure}

为了克服当前深度学习编译器对于硬件感知算法表达困难以及难以进行硬件相关优化的挑战，本研究希望能够提出一种硬件可感知的抽象。
在这层抽象上会尽可能暴露出性能有关的信息，这要起用户去基于特定硬件进行编程。
随后这层抽象将被被分析最后生成硬件上有效率的代码。
本研究的目标希望是通过以上抽象所映射出的硬件代码能够比当前深度学习编译器所生成出的代码性能更好。

本研究的总体路线如图 ~\ref{figure:abstract} 所示。
在最上层基于本研究提供的数据流图 API 使用调度中间表示进行描述，随后进行数据流图构建。
本研究提出了 AffineGraph 用于表示携带嵌套循环信息的数据流图，随后数据流图进行图降低（Graph Lower）将其转换为携带硬件内存信息的数据流图。
随后对图中的不同层级内存进行切块以增大数据重用和减少内存拷贝操作，最终针对具体硬件进行硬件映射与代码生成。
因此，本文的研究内容主要涉及三个方面，即：

\begin{itemize}
    \item 基于多内存层级数据流图的硬件感知抽象
    \item 基于分块策略的内核融合
    \item 高效的内核映射策略
\end{itemize}

\subsubsection{基于多内存层级数据流图的抽象}

当前，以 DAG 为基础的深度学习编译器出现的契机在于不断迭代的新模型架构的出现，例如 CNN~\cite{lecun1998cnn}，RNN~\cite{rumelhart1986rnn}，LSTM~\cite{hochreiter1997lstm} 以及 GAN~\cite{goodfellow2020gan} 等。
然而随着大语言模型的流行，当前的模型逐渐收敛到以 Transformer~\cite{nips/vaswani2017attention} 为核心的算法设计。
当前的算法设计更加注重于硬件感知度以及算法-硬件协同设计。
然而，基于 DAG 粗粒度的表示方法难以对这类算法进行描述。
因此需要一种合适粒度的抽象来对硬件感知算法进行描述，同时，基于 DAG 的描述方法也无法对硬件性能通过软件的分析方法挖掘到极致。
本课题的一个研究目标为设计一套抽象来对这种算法进行描述。

\textbf{预期目标：} 设计一个基于多内存层级数据流图的抽象，能够准确表达出硬件感知性算法并便于进行进一步的优化。

\subsubsection{基于分块策略的内核融合策略}

当前内核融合技术逐渐从基于算子的融合技术走向了基于 Tile 的融合技术。
其中 FlashAttention-v2~\cite{dao2023flashattention} 是一个在学术界和工业界都十分成功的例子。
FlashAttention-v2 通过 Tile 的融合技术将矩阵 Q, K, V 进行切分并不断加载到共享内存中并再次切块加载入寄存器内存中，随后执行在 Tensor Core 上的矩阵乘计算计算并进行归一化。
FlashAttention-v2 相比于传统的多头 Attention 具有极大的性能优势，增加了内存复用与并行度，减少了内存压力。
更进一步，FlashDecoding 通过将 K，V 进行进一步的切分并将其派发到不同的 block 中进行运行更进一步提高了并行度并可以将其运行在推理中。
除此之外，基于分块策略的内核融合技术也在其他算法中被应用，例如，FlashFFTConv~\cite{arxiv/fu2023flashfftconv} 探讨了如何基于分块融合对长卷积进行优化。
FlashNorm~\cite{arxiv/graef2024flash} 则探讨了如何对 RMSNorm 基于分块融合进行优化。

然而，基于 Tile 的内核融合技术在深度学习编译器中的应用仍然不是很多。
尽管 Roller~\cite{osdi/zhu2022roller}，Welder~\cite{osdi/shi2023welder}，PIT~\cite{sosp/zheng2023pit} 针对基于 Tile 的内核融合技术给出了一些尝试，然而它们依然是在基于 DAG 的编译器例如 TVM~\cite{tvm/chen2018tvm} 中进行讨论与实现。
然而由于 DAG 自身的局限性，基于 DAG 的 tilling 并没有办法去发挥所有硬件潜能。

因此，本课题的一个研究目标在于如何基于硬件感知的情况下进一步通过分块策略进行内核融合并进一步挖掘硬件性能。

\textbf{预期目标：} 将基于 Tile 的内核融合技术应用在多存层级的数据流图上进行优化，使得能够具有更多的内存复用以及挖掘出最大并行度。

\subsubsection{高效的内核映射策略}

当前的深度学习编译器通常将后端代码生成映射到硬件库（例如 CuBLAS, CuDNN~\cite{chetlur2014cudnn} 等）或者是基于手写的 CUDA Kernel 并在自动调优的协同下去自动生成。
然而硬件厂商库尽管在单算子上具有较好的性能，但是由于是黑盒，因此无法处理内核融合的问题；而基于手写与自动调优的 CUDA Kernel 则在一些情况下很难与优化的非常好的硬件厂商库的性能进行抗衡。

尽管 CUTLASS~\cite{CUTLASS} 提供了一个基于模版的编程框架，将一系列内核组件进行拆分，提供了 TiledMMA，TiledCopy，MMAAtom 等抽象。
然而，CUTLASS 过于极致的模版技术的运用以及非常灵活的抽象设计导致了编译器难以基于 CUTLASS 做分析及做代码生成。

ThunderKittens~\cite{arxiv/spector2024thunderkittens} 提供了一个合适的抽象粒度，它以线程束作为一个基准线，在线程束下的实现部分作为黑盒实现，在线程束之上作为白盒可以由用户进行设计。
然而，ThunderKittens 的限制在于它仅仅针对 H100 做了优化并且无法给予不同切块策略进行自动调优。
同时，由于 NVIDIA 硬件的高度复杂性，用户也很难针对自己的需求对 ThunderKittens 针对特定硬件进行优化与修改。

因此，本课题希望设计一套具有合适粒度抽象以及能够支持性能调优的代码生成方案。

\textbf{预期目标：} 基于 Tile 内核融合的中间表示设计一套具有合适粒度抽象以及能够支持性能调优的代码生成方案并进行有效率的代码生成。
生成的代码性能能够比当前主流的深度学习编译器要更好。

\subsection{本文组织结构}

本文共分为六章，各章节内容安排如下：

第一章：绪论。介绍深度学习编译器的发展背景，分析大语言模型和 GPU 架构演进对编译器带来的新挑战，阐述本研究的意义、主要研究内容及本文的组织结构。

第二章：相关技术概述。综述当前深度学习编译器的发展现状，分析现有技术（如基于DAG、多面体模型、硬件感知编译器以及基于 Tile 抽象的 GPU 编程 DSL 等）的优缺点，并指出存在的问题。

第三章：基于多内存层级数据流图的硬件感知抽象。详细介绍 AffineGraph IR 的设计，包括缓冲区节点、运算符节点、子图节点、仿射边、仿射映射及谓词执行等核心组件，并以 FlashAttention-2 为例展示 IR 的表达能力，最后通过与 CUDA、CuTe、ThunderKittens、Triton 和 TileLang 等主流框架的系统性对比评估 IR 的表达力。

第四章：基于分块策略的内核融合。阐述硬件感知优化策略，包括分块大小与并行度确定、解析式代价模型构建、Warp Layout 影响分析、层级执行模型以及高效内存访问优化。同时论述面向 NVIDIA Hopper 架构的可扩展性设计，并通过对内存受限算子的系统性分析验证 Micro-Task 执行模型的通用性。

第五章：高效的内核映射策略。介绍图降低与代码生成算法，以 GEMM 为案例展示完整的编译流程，包括层级化 IR 构建、布局推断与变换、CUDA 代码生成。同时展示系统实现细节，并给出在 NVIDIA A100 上涵盖内存操作、GEMM、融合 GEMM、FlashAttention 和 Block Sparse Attention 的全面实验评估，以及在 NVIDIA H800（Hopper 架构）上的可移植性验证实验。

第六章：总结和展望。总结本文的主要工作与贡献，讨论当前工作的局限性，并对未来的研究方向进行展望。
