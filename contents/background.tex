深度学习 (Deep Learning) 的发展对各个科学领域产生了深远的影响。它不仅在自然语言处理 (NLP) 和计算机视觉 (CV) 等人工智能领域展现出非凡的价值，而且在电子商务、智慧城市和药物研发等更广泛的应用中也取得了巨大成功。随着卷积神经网络 (CNN)~\cite{lecun1998cnn}、循环神经网络 (RNN)~\cite{rumelhart1986rnn}、长短期记忆 (LSTM)~\cite{hochreiter1997lstm} 和生成对抗网络 (GAN)~\cite{goodfellow2020gan} 等多功能深度学习模型的出现，简化各种 DL 模型的编程对于实现其广泛应用至关重要。随着工业界和学术界的不断努力，为了简化各种深度学习模型的实现，已经提出了几种流行的深度学习框架，例如 TensorFlow~\cite{osdi/abadi2016tensorflow}、PyTorch~\cite{nips/paszke2019pytorch}、MXNet~\cite{arxiv/chen2015mxnet} 等。同时，矩阵乘法等独特的计算特性激发了芯片架构师设计定制深度学习加速器的热情，以提高效率。例如 Google TPU、NVIDIA GPU、海思 NPU 等。除此之外，为了适应硬件的多样性，将计算有效地映射到深度学习硬件非常重要。在通用硬件上，高度优化的线性代数库（例如基本线性代数子程序 (BLAS) 库）（例如 MKL 和 cuBLAS）是深度学习模型高效计算的基础。以卷积操作为例，深度学习框架将卷积转换为矩阵乘法，然后调用 BLAS 库中的 GEMM 函数。

为了解决以上问题，深度学习社区提出了使用特定领域的编译器来解决问题，很快，工业界和学术界都提出了几种流行的深度学习编译器，例如 TVM~\cite{tvm/chen2018tvm}、XLA~\cite{xla} 等，深度学习编译器将深度学习框架中的模型定义作为输入，针对模型规范和硬件架构，模型定义和特定代码实现之间的转换得到了高度优化。

然而，随着大语言模型（LLM）的出现与流行，当前的深度学习算法设计逐渐更加注重于模型与硬件的协同设计，例如 Flash Attention~\cite{dao2022flashattention, dao2023flashattention, arxiv/graef2024flash}，Linear Attention~\cite{arxiv/yang2023gated} 等，难以被描述为大型的、规则的线性代数运算。当前的深度学习编译器通常使用有向无环图来描述算法，在早期这种表示方法可以用于扩展算法的并行度，然而基于有向无环图的表示方法无法对嵌套循环以及硬件信息进行描述，从而对非规则的硬件感知算法的表达与优化带来了限制。因此 Triton~\cite{pldi/tillet2019triton} 的出现能够给予 GPU 编程模型允许用户在多个内存层级上进行编程并做自动优化，然而 Triton 的编程模型过于耦合 NVIDIA GPU 同时无法基于寄存器进行编程，这也带来了一定的限制。

因此，本研究尝试提出一种能够表达当前硬件感知算法的表示方法并能够针对特定的硬件进行优化，最终生成有效率的硬件代码，这在当今的时代有着重要意义。