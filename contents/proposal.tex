\begin{figure}%[h]
    \centering
    \includegraphics[width=\textwidth]{figures/abstract.pdf}
    \caption{本研究系统总体路线}
    \label{figure:abstract}
\end{figure}

为了克服当前深度学习编译器对于硬件感知算法表达困难以及难以进行硬件相关优化的挑战，本研究希望能够提出一种硬件可感知的抽象，在这层抽象上会尽可能暴露出性能有关的信息，这要起用户去基于特定硬件进行编程。随后这层抽象将被被分析最后生成硬件上有效率的代码。本研究的目标希望是通过以上抽象所映射出的硬件代码能够比当前深度学习编译器所生成出的代码性能更好。

本研究的总体路线如图 ~\ref{figure:abstract} 所示，在最上层基于本研究提供的数据流图 API 使用调度中间表示进行描述，随后进行数据流图构建，本研究提出了 AffineGraph 用于表示携带嵌套循环信息的数据流图，随后数据流图进行图降低（Graph Lower）将其转换为携带硬件内存信息的数据流图，随后对图中的不同层级内存进行切块以增大数据重用和减少内存拷贝操作，最终针对具体硬件进行硬件映射与代码生成。因此，本文的研究内容主要涉及三个方面，即：

\begin{itemize}
    \item 基于多内存层级数据流图的硬件感知抽象
    \item 基于分块策略的内核融合
    \item 高效的内核映射策略
\end{itemize}

\subsection{基于多内存层级数据流图的抽象}

当前，以 DAG 为基础的深度学习编译器出现的契机在于不断迭代的新模型架构的出现，例如 CNN~\cite{lecun1998cnn}，RNN~\cite{rumelhart1986rnn}，LSTM~\cite{hochreiter1997lstm} 以及 GAN~\cite{goodfellow2020gan} 等，然而随着大语言模型的流行，当前的模型逐渐收敛到以 Transformer~\cite{nips/vaswani2017attention} 为核心的算法设计。当前的算法设计更加注重于硬件感知度以及算法-硬件协同设计。然而，基于 DAG 粗粒度的表示方法难以对这类算法进行描述。因此需要一种合适粒度的抽象来对硬件感知算法进行描述，同时，基于 DAG 的描述方法也无法对硬件性能通过软件的分析方法挖掘到极致。本课题的一个研究目标为设计一套抽象来对这种算法进行描述。

\textbf{预期目标：} 设计一个基于多内存层级数据流图的抽象，能够准确表达出硬件感知性算法并便于进行进一步的优化。

\subsection{基于分块策略的内核融合策略}

当前内核融合技术逐渐从基于算子的融合技术走向了基于 Tile 的融合技术。其中 FlashAttention-v2~\cite{dao2023flashattention} 是一个在学术界和工业界都十分成功的例子。FlashAttention-v2 通过 Tile 的融合技术将矩阵 Q, K, V 进行切分并不断加载到共享内存中并再次切块加载入寄存器内存中，随后执行在 Tensor Core 上的矩阵乘计算计算并进行归一化。FlashAttention-v2 相比于传统的多头 Attention 具有极大的性能优势，增加了内存复用与并行度，减少了内存压力。更进一步，FlashDecoding 通过将 K，V 进行进一步的切分并将其派发到不同的 block 中进行运行更进一步提高了并行度并可以将其运行在推理中。除此之外，基于分块策略的内核融合技术也在其他算法中被应用，例如，FlashFFTConv~\cite{arxiv/fu2023flashfftconv} 探讨了如何基于分块融合对长卷积进行优化。FlashNorm~\cite{arxiv/graef2024flash} 则探讨了如何对 RMSNorm 基于分块融合进行优化。

然而，基于 Tile 的内核融合技术在深度学习编译器中的应用仍然不是很多，尽管 Roller~\cite{osdi/zhu2022roller}，Welder~\cite{osdi/shi2023welder}，PIT~\cite{sosp/zheng2023pit} 针对基于 Tile 的内核融合技术给出了一些尝试，然而它们依然是在基于 DAG 的编译器例如 TVM~\cite{tvm/chen2018tvm} 中进行讨论与实现。然而由于 DAG 自身的局限性，基于 DAG 的 tilling 并没有办法去发挥所有硬件潜能。

因此，本课题的一个研究目标在于如何基于硬件感知的情况下进一步通过分块策略进行内核融合并进一步挖掘硬件性能。

\textbf{预期目标：} 将基于 Tile 的内核融合技术应用在多存层级的数据流图上进行优化，使得能够具有更多的内存复用以及挖掘出最大并行度。

\subsection{高效的内核映射策略}

当前的深度学习编译器通常将后端代码生成映射到硬件库（例如 CuBLAS, CuDNN~\cite{chetlur2014cudnn} 等）或者是基于手写的 CUDA Kernel 并在自动调优的协同下去自动生成。然而硬件厂商库尽管在单算子上具有较好的性能，但是由于是黑盒，因此无法处理内核融合的问题；而基于手写与自动调优的 CUDA Kernel 则在一些情况下很难与优化的非常好的硬件厂商库的性能进行抗衡。

尽管 CUTLASS~\cite{CUTLASS} 提供了一个基于模版的编程框架，将一系列内核组件进行拆分，提供了 TiledMMA，TiledCopy，MMAAtom 等抽象。然而，CUTLASS 过于极致的模版技术的运用以及非常灵活的抽象设计导致了编译器难以基于 CUTLASS 做分析及做代码生成。

ThunderKittens~\cite{arxiv/spector2024thunderkittens} 提供了一个合适的抽象粒度，它以 warp 作为一个基准线，在 warp 下的实现部分作为黑盒实现，在 warp 之上作为白盒可以由用户进行设计。然而，ThunderKittens 的限制在于它仅仅针对 H100 做了优化并且无法给予不同切块策略进行自动调优。同时，由于 NVIDIA 硬件的高度复杂性，用户也很难针对自己的需求对 ThunderKittens 针对特定硬件进行优化与修改。

因此，本课题希望设计一套具有合适粒度抽象以及能够支持性能调优的代码生成方案。

\textbf{预期目标：} 基于 Tile 内核融合的中间表示设计一套具有合适粒度抽象以及能够支持性能调优的代码生成方案并进行有效率的代码生成。生成的代码性能能够比当前主流的深度学习编译器要更好。