\subsection{整体研究路线}

\begin{figure}%[h]
    \centering
    \includegraphics[width=\textwidth]{figures/design-overview.pdf}
    \caption{AffineGraph 整体系统设计}
    \label{figure:design-overview}
\end{figure}
在图 ~\ref{figure:abstract} 展示了本研究系统的总体路线，在图 ~\ref{figure:design-overview} 中更细粒度地展示了每一个流程的内容，本课题的研究主要分为以下三个部分：

\begin{itemize}
    \item 设计硬件友好的中间表示与调度语言，能够表示当前流行的硬件感知算法。
    \item 基于多内存层级数据流图进行分析与切块。
    \item 基于分块技术的内核融合及代码生成。
\end{itemize}

在接下来的小节中将详细介绍本研究所设计的抽象与具体的实施方法。

\subsection{技术路线}

\subsubsection{基于多内存层级数据流图的抽象}

为了能够更好地表示当前一系列硬件感知性算法，本研究提出一种基于多内存层级的数据流图抽象，该抽象可以被描述为多个数据结构。本研究提出的数据结构如下所示：

多内存层级数据流图是一种用于处理数据流的数据结构，定义了数据流的整体结构与连接关系，该结构包括了嵌套数据流图，边，操作节点，缓存节点，块节点等数据结构。基于这些数据结构可以定义不同内存层级的数据流关系，这些组件的协同作用使得数据流的管理和操作更加灵活。

首先，嵌套数据流图作为核心数据结构，定义了数据流的整体结构和连接关系。Node 代表了图中的节点，每个节点可能对应于内存中的缓存节点(Buffer Node)，数据操作节点(Operator Node)或者块节点(Block Node)，当节点表示为块节点时，可以递归地进入并且进行代码生成。Edge 则是节点之间的连接线，描述了数据的流向和依赖关系。

缓存节点（Buffer Node）表示了一块具体的内存，包含数据类型、形状、内存层级等信息，针对不同的体系结构具有不同的定义。例如在 NVIDIA GPU 中的数据类型包含 Flash32, Float16，TF32 等等，内存层级包括全局内存，共享内存，寄存器内存等。

操作节点（Operator Node）表示一个对一块内存进行的具体的任务操作，例如进行 Unary 操作例如 Relu、Sigmod、Tanh 等，Element Wise 操作例如 Add，Power，BitAnd 等以及更为复杂的 Softmax，Broadcast 及 GEMM 等操作。

块节点（Block Node）是对子图的封装，块节点表示一个可以递归的数据并行任务，每一个块节点在语义上实际上表示有加载或者存储的操作发生，这通常会进行 tilling 操作以及携带嵌套循环。因此在块节点中会附带一个叫做仿射边（Affine Edge）的数据结构，用于表示对于内存节点的切块信息以及循环嵌套信息。

仿射边(Affine Edge)是一种携带访问映射（Affine Map）信息的边，其中边的源节点与目的节点均为缓存节点，用于指示数据的来源和去向。在 Affine Edge 中携带了 Access Map 用于表示对缓存节点的访问模式。

访问映射(Affine Map)的源与目的的缓存节点标志了内存层级，从而可以将 Affine Edge 映射到不同的操作上。例如，从共享内存到寄存器内存与从寄存器内存到共享内存的加载需要被映射到不同的任务（Task）中。Affine Map 是一种从多面体数学模型引申出来的数据结构，可以使用矩阵来表示加载与存储操作的嵌套循环信息与访问信息，有助于分析边的循环与访问模式，并可以进一步对于嵌套循环进行优化。图 ~\ref{figure:access-map} 是 Access Map 结构的示意。

图 ~\ref{figure:access-map} 表示一个 Affine Edge 的示意图，从共享内存缓存节点加载到寄存器内存缓存节点，中间的箭头表示 Affine Edge，包含了如何从共享内存到寄存器之间的传输规则，即 Affine Map。

\begin{figure}%[h]
    \centering
    \includegraphics[width=\textwidth]{figures/Access-Map.pdf}
    \caption{GEMM 中Affine Map示例}
    \label{figure:access-map}
\end{figure}

\begin{figure}%[h]
    \centering
    \includegraphics[width=\textwidth]{figures/GEMM.pdf}
    \caption{使用多内存层级数据流图表示 GEMM}
    \label{figure:gemm}
\end{figure}

图 ~\ref{figure:gemm} 展示了如何使用多层内存层级数据流图构建全内存级通用矩阵乘。图 ~\ref{figure:gemm} 左侧表示从全局内存到共享内存的数据流图。其中 gA, gB, gC 表示在全局内存中的缓冲节点，sA, sB, sC 表示在共享内存中的缓冲节点，缓冲节点中存储有内存信息。不同节点间的连线为仿射边，仿射边内部存储有仿射映射信息，用于表达加载与存储的访问模式。随后共享内存将被加载入一个块节点并进行后续的计算。

图 ~\ref{figure:gemm} 右侧表示块节点中的计算流程，共享内存缓冲节点将被切块加载入寄存器缓冲节点，并在执行 GEMM 计算后累加到 acc 中，最后被存储到 sC 共享内存中。
 
\subsubsection{基于分块策略的内核融合策略}

\begin{figure}%[h]
    \centering
    \includegraphics[width=\textwidth]{figures/TiledCUDA.pdf}
    \caption{GEMM 从共享内存到寄存器的分块策略}
    \label{figure:TiledCUDA}
\end{figure}

本研究希望提出一种基于分块的内核融合策略，当算法描述被抽象成多内存层级数据流图后，将数据基于硬件友好的方式自动分块并构建合适的迭代器与内存访问模式。

为了更好地说明本研究采用的技术方案，基于 GEMM 构建的分块策略可见图 ~\ref{figure:TiledCUDA}。图 ~\ref{figure:TiledCUDA} 展示了从共享内存到寄存器阶段的切块、加载、MMA 计算以及存储的流程。

为了能够在硬件上生成高效率的代码，需要按照一定的规则对于内存块进行切块，在本研究中 BaseTile 作为一个基础块的抽象作为单个 Warp 能够处理的最小单元。首先在 SharedTile 中按照一定的 ChunkShape 大小对共享内存进行切块，对共享内存的切块大小必须按照 BaseTile 的整倍数来构建。随后，共享内存按照切块规则被加载入寄存器内存中，RegTile 是基于 BaseTile 的一个嵌套数据结构，按照一定的布局对 BaseTile 进行排布。除此之外，当基于迭代器对共享内存进行遍历切块的时候需要指定 Warp Layout 以及重用规则以挖掘最大并行度。

当内存从 SharedTile 被加载入 RegTile 后将会在寄存器内存上进行 Tensor Core 矩阵乘操作，这需要遵守一定运算规则，运算完成后被存储回共享内存缓冲中。为了进行自动化切块，本研究也将引入一个基于 Auto Tunning 的 profiler 对于不同的切块策略进行选择，通过预定义的切块策略对于不同的算法描述进行自动选择从而筛选出最好的切块策略。

\subsubsection{高效的内核映射策略}

\begin{figure}%[h]
    \centering
    \includegraphics[width=\textwidth]{figures/tile-abstraction.pdf}
    \caption{Tile 分块抽象设计}
    \label{figure:tile-abstraction}
\end{figure}

\begin{figure}%[h]
    \centering
    \includegraphics[width=\textwidth]{figures/tiled_mma.pdf}
    \caption{$16 \times 8 \times 16$ Tensor Core 矩阵乘实现}
    \label{figure:tiled_mma}
\end{figure}

当对于数据流描述进行切块后，在最后一步需要将操作映射到具体的硬件上，本研究将对于微任务(Micro Task)进行封装。所谓的微任务指的是以一个 Warp 为单位执行的任务。图 ~\ref{figure:tiled_mma} 展示了在一个 BaseTile 上执行 mma 操作的微任务。当前一个 BaseTile 被设置为一个 $16 \times 16$ 的矩阵，因此为了完成一次矩阵乘需要执行两次 $16 \times 8 \times 16$ 的 Tensor Core 矩阵乘操作。除了矩阵乘操作外，本研究也对其他常见的计算做了实现，这里将不再一一进行赘述。

除了具体的微任务外，本研究也对 Tile 抽象做了设计与实现，如图 ~\ref{figure:tile-abstraction} 所示，在 RegTile 中存在数据信息以及布局信息，用于对具体的数据进行访问；TileIterator 中以指针为起点，同时支持多种访问模式，例如行访问，列访问以及索引访问。

最终，基于数据流图可以递归地进行映射到具体的硬件上。


\subsection{实验方案}

本研究首先要求用户将具体的 DNN 算法描述为本研究所提出的多内存层级数据流图的中间语言与数据结构中，这种表示形式能够表示算法的细粒度的结构与数据流，同时准确的描述出硬件细节。随后本研究将进一步将数据流图进行降低并进行 Auto Tunning 以进行分块，最终将其映射到具体的硬件上。

为了验证所设计的整体系统框架，需要进行详细的实验设计。本研究希望能对硬件感知性算法及大语言模型相关的算法进行友好的描述以及有效率的运行，因此本研究希望选取相关的算法进行描述与评估，例如通用矩阵乘、融合矩阵乘、FlashAttention 等算法，通过对这些算法进行描述与运行进行基准测试并与其他深度学习编译器与框架例如 TVM~\cite{tvm/chen2018tvm}、Pytorch~\cite{nips/paszke2019pytorch}、Triton~\cite{pldi/tillet2019triton}、TensorRT~\cite{TensorRT} 等进行对比来验证本研究在实际应用中的优势。


\subsection{可行性分析}

当前本课题已经成功将通用矩阵乘、融合矩阵乘、FlashAttention 使用多内存层级数据流图进行描述并将其映射到了 NVIDIA 安培架构的硬件上并可实际运行，初步验证了本研究的可行性。

\begin{table}[h] % 'h' 表示在此处插入
    \centering
    \caption{Execution time (ms) for GEMM（NVIDIA A100）} % 表格标题
    \begin{tabular}{cccccc} % 表格格式，'|' 表示有边框，'c' 表示居中对齐
        \hline % 横线
        [kM, kN, kK] & [kTM, kTN, kTK] & Cublas(ms) & Triton(ms) & Ours(ms)\\ % 列内容，用 '&' 分隔
        \hline % 横线
        [1024, 128, 64] & [64, 32, 32] & 0.0060 & 0.0857 & 0.0041 \\ \hline
        [2048, 128, 64] & [64, 64, 32] & 0.0072 & 0.0842 & 0.0048 \\ \hline
        [4086, 128, 64] & [64, 64, 32] & 0.0061 & 0.0797 & 0.0062 \\ \hline
        [8192, 128, 64] & [64, 64, 32] & 0.0076 & 0.0823 & 0.0084 \\ 
        \hline % 横线
    \end{tabular}
    \label{tab:gemm_bench} % 表格标签
\end{table}

\begin{table}[h] % 'h' 表示在此处插入
    \centering
    \caption{Execution time (ms) for Fused Two HGEMMs(NVIDIA A100)} % 表格标题
    \begin{tabular}{cccccc} % 表格格式，'|' 表示有边框，'c' 表示居中对齐
        \hline % 横线
        [kM, kN, kK] & [kTM, kTN, kTK] & Cublas(ms) & Triton(ms) & Ours(ms)\\ % 列内容，用 '&' 分隔
        \hline % 横线
        [2048, 128, 64, 64] & [64, 128, 64, 64] & 0.0117 & 0.0895 & 0.0076 \\ \hline
        [4096, 128, 64, 64] & [64, 128, 64, 64] & 0.0122 & 0.0930 & 0.0089 \\ \hline
        [8192, 128, 64, 64] & [128, 128, 64, 64] & 0.0148 & 0.0929 & 0.0125 \\ \hline
        [2048, 256, 64, 64] & [64, 128, 64, 64] & 0.0130 & 0.0888 & 0.0108 \\ \hline 
        [4096, 256, 64, 64] & [64, 128, 64, 64] & 0.0144 & 0.0998 & 0.0129 \\ \hline
        [8192, 256, 64, 64] & [128, 128, 64, 64] & 0.0179 & 0.0919 & 0.0169 \\
        \hline % 横线
    \end{tabular}
    \label{tab:fused_gemm_time} % 表格标签
\end{table}

\begin{figure}%[h]
    \centering
    \includegraphics[width=\textwidth]{figures/gemm_time.png}
    \caption{GEMM 执行时间}
    \label{figure:gemm_time}
\end{figure}


\begin{figure}%[h]
    \centering
    \includegraphics[width=\textwidth]{figures/fused_gemm_time.png}
    \caption{Fused Two GEMMs 执行时间}
    \label{figure:fused_gemm_time}
\end{figure}

图 ~\ref{figure:gemm_time} 及表 ~\ref{tab:gemm_bench} 展示了本技术与 CuBLAS 和 Triton 在 HGEMM 上的性能测试。 图 ~\ref{figure:fused_gemm_time} 及表 ~\ref{tab:fused_gemm_time} 展示了本技术与 CuBLAS 与 Triton 在 Fused Two HGEMMs 上的性能测试。可以观察到本研究所生成出的代码在 GEMM 上的性能基本与 CuBLAS 持平，超过了 Triton；生成出的融合两矩阵乘的代码超过了 CuBLAS，原因在于本研究所生成的内核对前两个计算的矩阵乘的结果进行内存驻留，减少内存拷贝次数。同时，本研究生成出的代码性能都要比 Triton 更好。该基准测试表明本研究所生成的代码在硬件感知型算法中更加具有优势。除此之外，本实验所评估的 Fused Two GEMMs 算法同时也是 FlashAttention-v2 算法的基础，为本研究在 FlashAttention-v2 算法的研究上奠定了基础。

本研究基于通用矩阵乘和融合两矩阵乘的性能测试初步验证了本研究所生成代码的性能优势，接下来将会进一步对性能优化并对更多的算法进行性能评估。

