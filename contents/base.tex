\subsection{科研基础}

\subsubsection{深度学习编译器的研究基础}

深度学习编译器的发展在近年来取得了显著进展，首先是端到端编译器例如 TVM、XLA 等的出现，随后出现了 Pytorch Dynamo~\cite{nips/paszke2019pytorch, asplos/ansel2024pytorch}，BladeDISC~\cite{zheng2023bladedisc} 用来做动态形状张量的优化。随着大模型的流行，Triton~\cite{pldi/tillet2019triton}，ThunderKittens~\cite{arxiv/spector2024thunderkittens} 等注重硬件体系结构的软件也逐渐流行，这都为本课题的研究提供了基础。

除此之外，本人在硕士期间一直关注深度学习编译器的发展，并为多个开源深度学习编译器例如 InfiniTensor~\cite{osdi/zheng2023einnet, osdi/wang2021pet}，等开源深度学习编译器提交过 PR。当前有一篇专利在投以及一篇论文准备投。

\subsubsection{分块融合技术的研究基础}

当前分块融合算法已经得到了一定的研究，例如在不断迭代的 FlashAttention~\cite{dao2022flashattention, dao2023flashattention, shah2024flashattention} 中探讨了如何对 Attention 算子在不同的 NVIDIA 架构下进行分块融合。FlashFFTConv~\cite{arxiv/fu2023flashfftconv} 探讨了如何基于分块融合对长卷积进行优化。FlashNorm~\cite{arxiv/graef2024flash} 则探讨了如何对 RMSNorm 基于分块融合进行优化，为本研究提供了理论支持。

除此之外，一些分块融合技术例如 Roller~\cite{osdi/zhu2022roller}，Welder~\cite{osdi/shi2023welder}，PIT~\cite{sosp/zheng2023pit} 也探讨了如何将基于分块的内核融合技术在深度学习编译器中进行支持，这也为本研究提供了支持。

个人也长期关注分块融合技术以及分块融合技术在编译器中的应用，曾经手写过多个分块融合内核，并且当前本研究所构建的深度学习编译器框架中已经可以生成 FlashAttention-v2 的内核。

\subsubsection{系统结构的研究基础}

个人长期关注体系结构的设计，尤其是 CPU、GPU 等架构。曾经手写过多个高性能 kernel 并给一些开源项目提过 PR。个人对 NVIDIA GPU 指令集有较深入的研究，熟悉 CUTLASS、CuBLAS，CuDNN 等硬件库，对 NVIDIA 架构中先进的设计例如 ldmatrix、swizzle、异步拷贝有深入的理解。

除此之外，个人也有许多系统设计与开发的经验，例如本人独立开发过 xv6-rust~\cite{xv6-rust}, hypocaust~\cite{hypocaust-2}, hypercraft~\cite{hypercraft} 等开源项目，并实际参与了 arceos~\cite{arceos} 项目的核心开发，这些项目均开源在了 github 上。

\subsection{科研条件}

\subsubsection{硬件设备支持}


本研究需要高性能 NVIDIA GPU 等硬件，实验室配备了 NVIDIA A100~\cite{nvidia-a100} 等高性能加速器硬件，这些设备为整个框架的实验提供了必要的计算资源。

\subsubsection{软件开发与实验环境}


实验室拥有完善的开发环境，包括高性能的服务器集群和支持多种深度学习框架（如 TensorFlow、PyTorch）的软件栈。个人也对当前主流的深度学习编译器以及 GPU 体系结构较为熟悉，这些都为本课题的研究提供了基础。