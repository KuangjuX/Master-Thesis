\section{总结和展望}

\subsection{总结}

随着深度神经网络的复杂度不断增长以及硬件架构日益异构化，传统的编译方法在将计算高效映射到专用执行单元方面面临着越来越大的挑战。
特别是大语言模型的兴起催生了 FlashAttention 等硬件感知算法，这些算法涉及复杂的多层级数据流动模式和精细的硬件资源管理，远超现有深度学习编译器中间表示的表达能力。
本文提出了一种新型的深度学习编译器框架 AffineGraph，通过层级化的图表示和精确的内存层级转换建模，实现了硬件感知的优化。
本文的主要工作和贡献如下：

\begin{enumerate}
    \item \textbf{基于多内存层级数据流图的硬件感知抽象。}
    提出了 AffineGraph IR，通过仿射变换来表达内存访问模式和计算依赖关系，为优化内存受限和计算受限操作提供了统一的抽象。
    该 IR 引入了三种节点类型（缓冲区节点、运算符节点和子图节点）和基于仿射映射的边，能够在单一表示中统一描述数据的存储位置、计算操作和跨内存层级的数据移动模式。
    IR 能够精确建模内存层级转换、嵌套循环结构和硬件特性（如 Bank Conflicts），同时支持通过谓词执行机制优雅地处理动态控制流。
    在表达力评估中，AffineGraph IR 以 18/18 的满分成为唯一在所有 9 项关键硬件特性上均实现完全自动支持的框架，实现 FlashAttention-2 仅需 170 行代码，相比 CUDA 减少约 90\% 的代码量。

    \item \textbf{基于分块策略的内核融合技术。}
    设计了硬件感知的优化策略，通过构建综合考虑计算能力、内存带宽、SRAM 与寄存器容量限制以及流水线效率的解析式代价模型，自动搜索最优的分块大小、Warp Layout 和流水线级数。
    代价模型引入了 L2 Cache 命中率建模、流水线掩盖效率和 Wave 量化效应等修正因子，能够在巨大的参数搜索空间中高效定位最优配置。
    提出了基于原子块（AtomicTile）和微任务（Micro-Task）的层级执行模型，将逻辑计算单元与物理执行计划分离，系统地管理代码生成的复杂性。
    通过对内存受限算子（Softmax、RMSNorm）的系统性分析，验证了 Micro-Task 模型在层级化归约策略、Cluster 级协作和寄存器资源管理方面的通用性和有效性。
    同时，论证了 AffineGraph 的核心抽象具有良好的架构可扩展性，能够自然地扩展到 NVIDIA Hopper 架构的 TMA、线程块集群和 Warpgroup 级操作等新特性。

    \item \textbf{高效的内核映射策略。}
    开发了完整的从 AffineGraph IR 到高性能 CUDA 代码的编译流程，包括图降低算法、布局推断与变换流水线、以及基于模板库的代码生成。
    图降低算法通过对数据流图的拓扑排序遍历，递归地将每个节点转换为对应的硬件操作。
    布局推断过程自动完成从全局内存到共享内存的向量化加载、共享内存的自适应 Swizzling 策略选择、以及寄存器级 BaseTile 抽象与 Tensor Core 的精确集成。
    系统采用 Python（用户接口）、Rust（图分析与优化）和 C++（代码生成）的多语言混合架构实现。
\end{enumerate}

实验结果表明，AffineGraph 在广泛的深度学习负载上取得了具有竞争力的性能。
在原始内存操作上，AffineGraph 相比 CUTLASS 取得了最高 33.3\% 的加速，验证了基于仿射分析的内存访问优化能力；
在 GEMM 操作上，相比 CUTLASS 平均达到 0.92 倍性能，相比 Triton 取得 1.14 倍加速，表明编译器生成代码已逼近手写优化水平；
在融合 GEMM 操作上展现了显著优势，相比 CUTLASS 最高取得 3.02 倍加速，相比 Triton 最高取得 3.26 倍加速，验证了仿射分析在自动识别跨算子数据复用机会方面的有效性；
在 FlashAttention 上，相比 FlashAttention-2 最高取得 3.72 倍加速，相比 Triton 最高取得 3.06 倍加速，尤其在短序列场景下优势显著；
在 Block Sparse Attention 上，展现了处理不规则计算模式的通用能力，相比 FlashAttention-2 取得了 2.5--9.4 倍的加速。
此外，在 NVIDIA H800（Hopper 架构）上的实验表明，AffineGraph 的核心抽象——层级化内存建模和基于仿射的访问模式分析——能够自然地扩展到更新的 GPU 架构。
Hopper 原型在内存受限操作上达到了接近峰值的内存带宽（约 89\%），仅需指令级别的适配即可保持相同的 IR 设计，验证了本课题方法的架构可扩展性。

\subsection{局限性讨论}

尽管 AffineGraph 在多种深度学习算子上取得了具有竞争力的性能，但当前工作仍存在一些局限性，需要在未来的研究中加以解决。

\subsubsection{动态形状支持的局限}

当前 AffineGraph IR 的仿射映射和迭代域定义要求在编译时确定张量的形状和分块大小。
这意味着对于动态形状的工作负载（如变长序列的注意力计算、动态批量大小等），需要为每种形状配置分别进行编译，或者采用保守的静态上界估计。
虽然 JIT（Just-In-Time）编译在一定程度上缓解了这一问题——当运行时形状确定后即时触发编译——但频繁的重编译会引入额外的延迟开销。
相比之下，PyTorch 2.0 的 \texttt{torch.compile}~\cite{asplos/ansel2024pytorch} 通过动态形状守卫（Dynamic Shape Guards）和符号化形状推断提供了更灵活的动态形状支持。
未来可以考虑在 AffineGraph IR 中引入参数化的仿射映射，使得分块大小和迭代域边界可以作为运行时参数，从而在保持仿射分析能力的同时支持动态形状。

\subsubsection{硬件后端的覆盖范围}

当前 AffineGraph 的代码生成后端仅支持 NVIDIA GPU（Ampere 和 Hopper 架构）。
虽然 AffineGraph IR 的核心抽象（仿射映射、层级化内存建模）在设计上具有硬件无关性，但实际的代码生成模板和硬件特定优化（如 Swizzle 策略、Tensor Core 指令选择）是针对 NVIDIA GPU 定制的。
将 AffineGraph 扩展到 AMD GPU（如 MI300X，采用 CDNA 架构和 Matrix Core）或其他类型的加速器（如 Google TPU、华为昇腾 NPU）需要：（1）为目标硬件定义新的内存层级模型和计算原语；（2）开发对应的代码生成模板；（3）调整代价模型中的硬件参数。
这些工作虽然在 AffineGraph 的架构设计中是可行的，但需要大量的工程投入和硬件专业知识。

\subsubsection{自动调优能力的局限}

当前 AffineGraph 的配置搜索主要依赖解析式代价模型进行离线评估，然后通过 JIT 编译对 Top-K 配置进行实际性能测量。
这种方法在搜索效率上优于纯黑盒的自动调优（如 AutoTVM 需要数千次实际运行），但代价模型的预测精度受限于其对硬件行为的简化建模。
例如，代价模型目前未考虑指令级流水线冲突、寄存器分配对占用率的精确影响、以及 L1 Cache 的行为等因素。
此外，对于融合操作的代价建模更加复杂，因为融合后的内核涉及多个计算阶段之间的资源竞争和流水线交互。
未来可以考虑引入基于机器学习的代价模型（如 TLP~\cite{nips/chen2018learning} 中的方法），通过少量实际测量数据来校准解析式模型的预测。

\subsubsection{算子覆盖范围}

当前 AffineGraph 的评估主要集中在矩阵乘法（GEMM）、注意力机制（FlashAttention、Block Sparse Attention）和逐元素操作（Softmax、RMSNorm）等核心算子上。
对于大语言模型中的其他重要操作，如嵌入层（Embedding）、采样操作（Top-K/Top-P Sampling）、量化/反量化操作、以及混合专家模型（MoE）中的路由和稀疏 GEMM 等，AffineGraph 的支持尚不完善。
此外，当前的评估以单算子为主，尚未进行端到端的完整模型（如 Llama、Qwen 等）部署与评估。
端到端场景涉及算子间的数据传递、内存分配策略、以及多内核的调度协同等问题，这些都是单算子评估无法覆盖的。


\subsection{下一步研究计划}

为了进一步完善 AffineGraph 框架并扩展其应用范围，下一步的研究将主要集中在以下几个方面：

\begin{enumerate}
    \item \textbf{完善 Hopper 架构的完整后端集成。}
    当前 Hopper 架构的支持以原型验证为主，已在内存受限操作（Softmax、RMSNorm）上验证了可行性。
    未来计划将 Hopper 代码生成后端完整集成到编译器中，支持计算密集型操作（如 GEMM 和 FlashAttention）在 Hopper 上的自动优化。
    具体而言，需要实现 TMA 描述符的自动生成、Warpgroup 级 MMA（\texttt{wgmma}）指令的集成、以及线程块集群的自动配置。
    此外，Hopper 架构引入的异步事务屏障（Async Transaction Barrier）为更精细的软件流水线提供了硬件支持，AffineGraph 的 Micro-Task 模型可以自然地利用这一特性来实现更高效的计算与通信重叠。

    \item \textbf{扩展后端支持与算子库。}
    未来计划将 AffineGraph 扩展到其他硬件后端，例如 AMD GPU（MI300X）或其他类型的深度学习加速器。
    AffineGraph IR 的核心抽象——仿射映射和层级化内存建模——在设计上具有硬件无关性，扩展到新硬件主要需要定义新的内存层级模型、计算原语和代码生成模板。
    同时，将进一步丰富算子库，以支持更多种类的深度学习操作，包括卷积操作、量化 GEMM、MoE 路由与稀疏计算等。
    特别是对于低精度计算（INT8、INT4、FP8 等），AffineGraph 的仿射映射可以自然地编码量化和反量化操作的数据布局变换，为低精度推理优化提供编译器级别的支持。

    \item \textbf{端到端模型部署与评估。}
    当前的评估主要集中在核心算子上。
    下一步将进行端到端（End-to-End）的完整模型（如 Llama, Qwen 等）部署与评估，以验证 AffineGraph 在真实应用场景下的综合性能和实用性。
    端到端部署需要解决多个技术挑战：（1）算子间的内存分配与数据传递优化；（2）多内核的调度与流水线协同；（3）与深度学习框架（如 PyTorch）的无缝集成。
    此外，还计划在推理场景下评估 AffineGraph 的性能，包括自回归生成中的 KV Cache 管理和动态批量处理等。

    \item \textbf{代价模型的增强与自动化程度提升。}
    未来计划从两个方面增强代价模型：一方面，引入更精确的硬件行为建模，包括指令级流水线模拟、寄存器分配对占用率的影响、以及 L1/L2 Cache 行为的建模；另一方面，探索基于机器学习的代价模型校准方法，通过少量实际测量数据来修正解析式模型的预测偏差。
    此外，还计划提升编译器的自动化程度，减少用户需要手动指定的参数（如初始分块策略），使得 AffineGraph 能够从高层次的算法描述自动生成接近最优的硬件代码。
\end{enumerate}
