\section*{摘要}

随着深度学习模型的不断发展，特别是大语言模型（LLM）的兴起，模型结构日益复杂，对硬件性能的要求也越来越高。现有的深度学习编译器在处理硬件感知算法（如 FlashAttention）时，往往难以充分发挥硬件的潜力。基于有向无环图（DAG）的中间表示无法有效描述嵌套循环和复杂的内存层级交互，而基于多面体模型的编译器在硬件特定优化方面存在局限性。

为了解决上述问题，本文提出了一种名为 AffineGraph 的新型编译器框架。该框架引入了基于多内存层级数据流图的硬件感知抽象，通过仿射变换精确建模内存层级转换和计算模式。在此基础上，本文提出了基于分块策略的内核融合技术，通过自动化的图变换和解析式代价模型，探索最优的分块和融合策略，并论证了该设计向 NVIDIA Hopper 等新架构的可扩展性。最后，本文设计了高效的内核映射策略，通过基于原子块和微任务的层级执行模型，将优化后的数据流图映射为高性能的 CUDA 代码。

实验结果表明，AffineGraph 在 GEMM、融合 GEMM（最高 3.02 倍加速）、FlashAttention（最高 3.72 倍加速）以及 Block Sparse Attention 等关键负载上，均取得了与手写优化代码相当甚至更好的性能。此外，在 NVIDIA H800（Hopper 架构）上的实验验证了 AffineGraph 的架构可移植性，在内存受限操作上达到了接近峰值的内存带宽利用率（约 89\%）。

\textbf{关键词：} 深度学习编译器，硬件感知，数据流图，内核融合，代码生成

